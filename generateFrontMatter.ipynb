{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import json\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-palmer",
   "metadata": {},
   "source": [
    "# Read Proceedings Information\n",
    "Update the `proceedingsInfo.csv` file according to your conference and the filed downloaded from PCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCommittee = pd.read_csv(\"./data/committee.csv\")\n",
    "dfCommittee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVenues = pd.read_csv(\"./data/proceedingsInfo.csv\")\n",
    "dfVenues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-mortality",
   "metadata": {},
   "source": [
    "# Generate Committee Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommittee(PCSId):\n",
    "    path = f\"./data-PCS/{PCSId}_committee.csv\"\n",
    "    if (not os.path.isfile(path)):\n",
    "        print(f\"{PCSId} has no committee file\")\n",
    "        return []\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df[\"Reviews assigned\"] != 0]\n",
    "    \n",
    "    if (len(df) == 0):\n",
    "        print(f\"{PCSId} has no reviewers\")\n",
    "        return []\n",
    "\n",
    "    lstText = []\n",
    "    df[\"Family name\"] = df[\"Family name\"].str.title()\n",
    "    df[\"First name\"] = df[\"First name\"].str.title()\n",
    "    #df[\"Middle name\"] = df[\"Middle name\"].str.title()\n",
    "    df = df.sort_values([\"Family name\", \"First name\", \"Middle initial\"])\n",
    "\n",
    "    for i, e in df.iterrows():\n",
    "        for i in range (1,7):\n",
    "            if isinstance(e[f\"Affil {i} Institution\"], str):\n",
    "                break\n",
    "        aff = \"\"\n",
    "        if isinstance(e[f\"Affil {i} Institution\"], str):\n",
    "            aff = f'{e[f\"Affil {i} Institution\"]}, {e[f\"Affil {i} Country\"]}'\n",
    "\n",
    "        aff = aff.replace(\"&\", \"\\\\&\")\n",
    "        if isinstance(e[\"Middle initial\"], str):\n",
    "            lstText.append(f'{e[\"First name\"]} {e[\"Middle initial\"]} {e[\"Family name\"]}, \\\\emph{{{aff}}}\\\\\\\\')\n",
    "        else:\n",
    "            lstText.append(f'{e[\"First name\"]} {e[\"Family name\"]}, \\\\emph{{{aff}}}\\\\\\\\')\n",
    "\n",
    "    if (len(df) > 50):   \n",
    "        lstText.insert(0, \"\\\\begin{multicols}{2}\")\n",
    "        lstText.append(\"\\end{multicols}\")\n",
    "    else:\n",
    "        \n",
    "        lstText.insert(0, \"%\\\\begin{multicols}{2}\")\n",
    "        lstText.append(\"%\\end{multicols}\")\n",
    "    lstText.append(\"\")\n",
    "    return lstText\n",
    "\n",
    "def getReviews(PCSId):\n",
    "    path = f\"./data-PCS/{PCSId}_reviewers.csv\"\n",
    "    if (not os.path.isfile(path)):\n",
    "        print(f\"{PCSId} has no reviwer file\")\n",
    "        return []\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df[\"Reviews assigned\"] != 0]\n",
    "    \n",
    "    if (len(df) == 0):\n",
    "        print(f\"{PCSId} has no reviewers\")\n",
    "        return []\n",
    "    \n",
    "    # Ensure that all names start with a capital first latter.\n",
    "    df[\"Family name\"] = df[\"Family name\"].apply(lambda x: x[0].title()+x[1:] if len(x)>2 else x)\n",
    "    df[\"First name\"] = df[\"First name\"].apply(lambda x: x[0].title()+x[1:] if len(x)>2 else x)\n",
    "    if \"Middle name\"in df.columns:\n",
    "        df[\"Middle name\"] = df[\"Middle name\"].apply(lambda x: x[0].title()+x[1:] if len(x)>2 else x)\n",
    "    \n",
    "    df = df.sort_values([\"Family name\", \"First name\", \"Middle initial\"])\n",
    "    lstText = []\n",
    "    lstText.append(\"\\\\begin{multicols}{3}\")\n",
    "    for i, e in df.iterrows():\n",
    "        if isinstance(e[\"Middle initial\"], str):\n",
    "            lstText.append(f'{e[\"First name\"]} {e[\"Middle initial\"]} {e[\"Family name\"]}\\\\\\\\')\n",
    "        else:\n",
    "            lstText.append(f'{e[\"First name\"]} {e[\"Family name\"]}\\\\\\\\')\n",
    "    lstText.append(\"\\\\end{multicols}\")\n",
    "    lstText.append(\"\")\n",
    "    return lstText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastPosition = \"\"\n",
    "\n",
    "lstExport = []\n",
    "lstExport.append(\"% Please list all organization committee members and their respected roles below. The best source to fill in this document is the conference webpage.\")\n",
    "\n",
    "for i, e in dfCommittee.iterrows():\n",
    "    \n",
    "    if (lastPosition != e.Position):\n",
    "        lastPosition = e.Position\n",
    "        lstExport.append(\"\")\n",
    "        lstExport.append(f\"\\subsection{{{e.Position}}}\")\n",
    "    \n",
    "    lstExport.append(f'{e.Name}, \\\\emph{{{e.Affiliation}, {e.Country}}}\\\\\\\\')\n",
    "    \n",
    "if (len(lstExport) > 0):\n",
    "        with open(f'committee/committee-organizer.tex', 'w') as fp:\n",
    "            fp.write('\\n'.join(lstExport))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in dfVenues.iterrows():\n",
    "    lstExport = []\n",
    "    lstExport.append(\"% If this venue/track has subcommittees, you might want to split the \\subsection{Committee Member} into different \\subsubsections for the different committees.\")\n",
    "    lstExport.append(\"\")    \n",
    "    dfX = dfCommittee[dfCommittee.VenueId == e.VenueId]\n",
    "\n",
    "    if (len(dfX) > 0):\n",
    "        lstExport.append(f\"\\\\subsection{{{e.Name} Chairs}}\")\n",
    "        for j, c in dfX.iterrows():\n",
    "            lstExport.append(f\"{c.Name}, \\emph{{{c.Affiliation}, {c.Country}}}\\\\\\\\\")\n",
    "    else: \n",
    "        print(f\"WARNING: {e.Name} has no chairs assigned to it. use the VenueId '{e.VenueId}' and assign them in the ./data/committee.csv to the respective chair(s)\")\n",
    "    \n",
    "    lstExport.append(\"\")\n",
    "    lstExport.append(\"\")\n",
    "    \n",
    "    commitee = getCommittee(e.PCSId)\n",
    "    if (len(commitee) > 0):\n",
    "        lstExport.append(f\"\\subsection{{{e.NameCommittee}}}\")\n",
    "        lstExport.extend(commitee)\n",
    "\n",
    "    reviewers = getReviews(e.PCSId)\n",
    "    if (len(reviewers) > 0):\n",
    "        lstExport.append(f\"\\subsection{{{e.NameReviewers}}}\")\n",
    "        lstExport.extend(reviewers)\n",
    "    \n",
    "    if (len(lstExport) > 0):\n",
    "        with open(f'committee/committee-{e.VenueId}.tex', 'w') as fp:\n",
    "            fp.write('\\n'.join(lstExport))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-basics",
   "metadata": {},
   "source": [
    "# Loading ACM E-Rights CSV File\n",
    "To get the CSV, navigate to https://cms.acm.org/cms_proceeding_papers_public.cfm?proceedingID=YOURPROCEEDINGSID&confID=YOURCONFERENCEID, at the top left press the `Create CSV` button and copy-and-pasted the content from the new window into the `export.csv` in the `data-erights` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-porcelain",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"./data-erights/export.csv\", 'r')\n",
    "lines = file1.readlines()\n",
    "ids = []\n",
    "lstLines = []\n",
    "for x in lines[1:]:\n",
    "    e = x[1:].split('\",\"')\n",
    "\n",
    "    if (not(\"WITHDRAWN\" in x)):\n",
    "        lstLines.append(e)\n",
    "        ids.append(x.split(\",\")[0].replace('\"', \"\"))\n",
    "    else:\n",
    "        print(\"WITHDRAWN\", x.split(\",\")[0].replace('\"', \"\"))\n",
    "    \n",
    "dfACM = pd.DataFrame(lstLines)\n",
    "dfACM.columns = [\"ID\",\"Title\",\"Author\",\"Email\",\"DL Paper Type\",\"Rights Granted\",\"Third Party\",\"Aux. Material\",\"Video Recording\",\"Artistic Images\",\"Govt. Employees\",\"Open Access\",\"DOI\",\"Authorizer\",\"Statement\",\"CC License\",\"Non-ACM Copyright\"]\n",
    "dfACM['Email'] = dfACM.Email.apply(lambda x: x.split(\" \")[0])\n",
    "dfACM['Signed'] = dfACM['Non-ACM Copyright'].apply(lambda x: x[:-3]) != \"load For\"\n",
    "dfACM.Title = dfACM.Title.apply(lambda x: x.split(\" \\\\setcopyright{\")[0])\n",
    "dfACM[\"Prefix\"] = dfACM.ID.apply(lambda x: re.sub(r'[0-9]', '', x))\n",
    "\n",
    "dfACM[\"TitleRaw\"] = dfACM.Title.str.replace('\"', '')\n",
    "dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('[', '', regex=False)\n",
    "\n",
    "## Might needs to be applied to fix LaTex issues.\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('`', '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('â€œ', '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace(\"'\", '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('(', '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.replace('#', '', regex=False)\n",
    "#dfACM[\"TitleRaw\"] = dfACM.TitleRaw.str.lower()\n",
    "\n",
    "# Remove duplicates, this is possible when the contact authors can not sign the copyright for all authors.\n",
    "dfACM = dfACM.drop_duplicates(\"ID\") \n",
    "\n",
    "dfACM = dfACM[[\"ID\", \"Prefix\", \"Title\", \"Author\", \"DOI\"]]\n",
    "\n",
    "#dfACM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(dfACM, dfVenues[[\"Prefix\", \"Name\", \"VenueId\"]], on=\"Prefix\")\n",
    "\n",
    "myOrder = CategoricalDtype(\n",
    "    dfVenues.sort_values(\"Order\").Prefix.to_list(), \n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "df.Prefix = df.Prefix.astype(myOrder)\n",
    "df = df.sort_values([\"Prefix\", \"Title\"])\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-portuguese",
   "metadata": {},
   "source": [
    "# Load Session Data From QOALA\n",
    "QOALA is the SIGCHI tool to schedule conferences, see https://services.sigchi.org/qoala. Exort the session data from QOALA as `.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data-QOALA/export.json\", 'r') as f:\n",
    "    qoala = json.load(f)\n",
    "    \n",
    "if (qoala[\"schemeVersion\"] != 7):\n",
    "    print(\"WARNING: This script was not tested with the QOALA expert scheme version. It might not fully working.\")\n",
    "    \n",
    "dfQPapers = pd.DataFrame(qoala[\"contents\"])\n",
    "\n",
    "if len(dfQPapers) > 0:\n",
    "\n",
    "    dfX = dfQPapers[dfQPapers.sessionIds.apply(lambda x: len(x) > 1)]\n",
    "    if len (dfX) > 0:\n",
    "        print(f'WARNING: The following papers are in more than one session: {dfX.importedId.to_list()}')\n",
    "\n",
    "    dfQPapers.sessionIds = dfQPapers.sessionIds.apply(lambda x: x[0])\n",
    "\n",
    "    dfQSessions = pd.DataFrame(qoala[\"sessions\"])\n",
    "    dfQSessions = dfQSessions.rename(columns={\"id\":\"sessionId\", \"name\":\"SessionName\"})\n",
    "\n",
    "    dfQoala = pd.merge(dfQPapers, dfQSessions[[\"sessionId\", \"SessionName\"]], left_on=\"sessionIds\", right_on=\"sessionId\", how=\"left\")[[\"importedId\", \"title\", \"SessionName\"]]\n",
    "    dfQoala[\"PCSId\"] = dfQoala.importedId.apply(lambda x: x.split(\"-\")[0])\n",
    "    dfQoala[\"IdRaw\"] = dfQoala.importedId.apply(lambda x: x.split(\"-\")[1])\n",
    "\n",
    "    myMap = dfVenues.set_index(\"PCSId\")[\"Prefix\"].to_dict()\n",
    "    dfQoala[\"Prefix\"] = dfQoala.PCSId.map(myMap)\n",
    "\n",
    "    dfQoala[\"ID\"] = dfQoala[\"Prefix\"] + dfQoala[\"IdRaw\"]\n",
    "    dfQoala.head()\n",
    "else:\n",
    "    print(\"WARNING: QOALA data for the papers is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\"SessionName\" in df.columns):\n",
    "    del df[\"SessionName\"]\n",
    "df = pd.merge(df, dfQoala[[\"ID\", \"SessionName\"]], on=\"ID\", how=\"outer\")\n",
    "\n",
    "for i, e in dfVenues.iterrows():\n",
    "    if (not e.UseQOALASessions):\n",
    "        df.loc[df.Prefix == e.Prefix, \"SessionName\"] = e.Name\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-passion",
   "metadata": {},
   "source": [
    "# Generate the TOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstExport = []\n",
    "lstExport.append(\"%% This file lists all items that are in the proceedings in the ACM DL. This again varies depending on if you have a companion proceedings or not.\")\n",
    "lstExport.append(\"%% Note: For conferences that will publish the full papers in PACMHCI, then the full papers are not to be listed here.\")\n",
    "lstExport.append(\"\")\n",
    "lastPrefix = \"\"\n",
    "counterTOC = 1\n",
    "\n",
    "tapsTOC=[]\n",
    "lstAuthorIndex = []\n",
    "lstDetails = []\n",
    "for j, f in dfVenues.sort_values(\"Order\").iterrows():\n",
    "    dfTrack = df[df.Prefix == f.Prefix]\n",
    "    \n",
    "    counter = 1\n",
    "    lastSessionName = \"\"\n",
    "    \n",
    "    lstExport.append(f'\\\\subsection{{{f.Name}}}')\n",
    "    \n",
    "    if (f.UseQOALASessions):\n",
    "        dfTrack = dfTrack.sort_values([\"SessionName\", \"Name\"])\n",
    "    else: \n",
    "        dfTrack = dfTrack.sort_values([\"Name\"])\n",
    "    \n",
    "    for i, e in dfTrack.iterrows():\n",
    "        \n",
    "        if (lastSessionName != e.SessionName) & (f.UseQOALASessions):\n",
    "            lastSessionName = e.SessionName\n",
    "            if (counter != 1):\n",
    "                lstExport.pop()\n",
    "                lstExport.append(f'\\\\end{{enumerate}}')\n",
    "                lstExport.append(\"\")\n",
    "            lstExport.append(f'\\\\subsubsection{{{e.SessionName}}}')\n",
    "            lstExport.append(f'\\\\begin{{enumerate}}')\n",
    "        elif (counter == 1):\n",
    "            lstExport.append(f'\\\\begin{{enumerate}}')\n",
    "        \n",
    "        lstExport.append(f'%PCS ID: {e.ID}')\n",
    "        TOC_ID = f\"{e.VenueId.upper()}{counter:03}\"\n",
    "        lstExport.append(f'\\\\item[\\\\href{{{e.DOI}}}{{\\\\textbf{{{TOC_ID}}}}}]')\n",
    "\n",
    "        xx = f'\\\\href{{{e.DOI}}}{{\\\\textbf{{{e.Title}}}}}\\\\\\\\'\n",
    "        lstExport.append(xx.replace(\"&\", \"\\\\&\").replace(\"#\", \"\\\\#\"))\n",
    "        for x in e.Author.split(\";\"):\n",
    "            x = x.split(\":\")\n",
    "            xx = f\"{x[0]}, \\\\emph{{({x[1]})}}\\\\\\\\\"\n",
    "            lstAuthorIndex.append([x[0], TOC_ID])\n",
    "            lstExport.append(xx.replace(\"&\", \"\\\\&\").replace(\"#\", \"\\\\#\").replace(\",\", \", \").replace(\"  \", \" \"))\n",
    "\n",
    "        lstExport.append(\"\")\n",
    "\n",
    "        counter = counter + 1\n",
    "        counterTOC = counterTOC + 1\n",
    "\n",
    "        lstDetails.append({\"ID\": e.ID, \"TOC_ID\":TOC_ID, \"Order\":counterTOC})\n",
    "    \n",
    "    lstExport.pop()\n",
    "    lstExport.append(f'\\\\end{{enumerate}}')\n",
    "    lstExport.append(\"\")\n",
    "    lstExport.append(\"\")\n",
    "\n",
    "if (len(lstExport) > 0):\n",
    "    with open(f'./content/content.tex', 'w') as fp:\n",
    "        fp.write('\\n'.join(lstExport))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-cradle",
   "metadata": {},
   "source": [
    "# Generate the TOC for APTARA\n",
    "This puts the TOC entries into \"sessions.\" Sessions are the structure ACM used in the ACM DL; for example, see the structure here on the left side: https://dl.acm.org/doi/proceedings/10.1145/3544548\n",
    "The list is to be sent via email to APTARA so they can sort them accordingly and prepare for the ACM upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAptaraExport = pd.merge(df, pd.DataFrame(lstDetails), on=\"ID\")\n",
    "dfAptaraExport = dfAptaraExport.rename(columns={\"Name\": \"SessionName\"})\n",
    "dfAptaraExport = dfAptaraExport[[\"Order\", \"ID\", \"TOC_ID\", \"Title\", \"SessionName\"]]\n",
    "dfAptaraExport = dfAptaraExport.sort_values(["SessionName", "TOC_ID"])\n",
    "dfAptaraExport.to_csv(\"./export/TOC-for-APTARA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-tumor",
   "metadata": {},
   "source": [
    "# Generate Author Index for the Back Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAI = pd.DataFrame(lstAuthorIndex)\n",
    "dfAI.columns = [\"Name\", \"Submission\"]\n",
    "dfAI = dfAI.groupby(\"Name\").Submission.apply(lambda x: \", \".join(x))\n",
    "dfAI = dfAI.reset_index()\n",
    "dfAI[\"NameRaw\"] = dfAI.Name.str.replace('\"', '')\n",
    "dfAI[\"NameRaw\"] = dfAI.NameRaw.apply(lambda x: unidecode(x))\n",
    "dfAI[\"NameRaw\"] = dfAI.NameRaw.str.replace('[', '', regex=False)\n",
    "dfAI[\"NameRaw\"] = dfAI.NameRaw.str.lower()\n",
    "dfAI = dfAI.sort_values(\"NameRaw\")\n",
    "\n",
    "with open(f'./content/index.tex', 'w') as fp:\n",
    "    fp.write('\\\\begin{multicols}{2}\\n')\n",
    "    for i, e in dfAI.iterrows():\n",
    "        fp.write(f'{e.Name} \\dotfill {e.Submission}\\\\\\\\\\n')\n",
    "\n",
    "    fp.write('\\\\end{multicols}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
